{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b045dff-5990-447b-aaf2-1617b72452da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.4.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.17.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0,>=2.3->stable-baselines3) (12.8.61)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->stable-baselines3) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install below dependencies\n",
    "# !pip install gymnasium\n",
    "# !pip install pygame\n",
    "# !pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93876ddf-7097-4e10-9102-af775f78831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 18:17:23.817982: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-13 18:17:23.832797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739470643.852466    4279 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739470643.862167    4279 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 18:17:23.911208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from typing import Dict, Any, Optional, Callable\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register, registry\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f78ddd-26d1-449c-84d1-fb4429637e99",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for COLREG-Compliant Autonomous Surface Vehicle Simulation\n",
    "*Project by Stanislav Dobrev for the SoftUni Deep Learning Course*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c152ef-3d3c-4cdd-bbb8-7d9dde30bf2b",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Maritime transportation plays a critical role in global trade, accounting for over 80% of the world's goods movement. As automation advances, the development of autonomous surface vehicles (ASVs) is becoming increasingly relevant for safer and more efficient navigation. However, ensuring that ASVs comply with the International Regulations for Preventing Collisions at Sea (COLREGs) remains a significant challenge.\n",
    "\n",
    "This study presents a partially implemented COLREG-compliant ASV simulation, where a reinforcement learning (RL) agent, trained using Proximal Policy Optimization (PPO), is responsible for navigating toward a waypoint while avoiding potential collisions. The agent is designed to handle static obstacles, head-on encounters, and crossing targets in a dynamic environment. Unlike many previous approaches that incorporate explicit waypoint tracking algorithms or cross-track error corrections, this implementation relies solely on a reward-based learning mechanism to shape the agent’s decision-making process.\n",
    "\n",
    "By leveraging a structured reward function, the ASV learns collision avoidance behaviors without predefined path-following rules, demonstrating an emergent compliance with fundamental COLREG principles. The study provides insights into the effectiveness of reinforcement learning in maritime navigation and highlights the potential and limitations of a purely reward-driven training paradigm. Further improvements and extensions, including enhanced optimization strategies and real-world applicability, are discussed as part of future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f60a5c-b09d-4ea4-902b-5b05dcc8d665",
   "metadata": {},
   "source": [
    "## 1. Introduction and Problem Definition\n",
    "### 1.1 The Future of Autonomous Ships\n",
    "The maritime industry is undergoing a transformative shift with the advent of autonomous ships, driven by advances in artificial intelligence, automation, and reinforcement learning. The potential benefits of autonomous surface vehicles (ASVs) include increased efficiency, reduced operational costs, and enhanced safety by minimizing human errors. Major stakeholders, including research institutions, shipping companies, and regulatory bodies, are actively exploring autonomous navigation solutions. However, the implementation of fully autonomous ships remains in its early stages, facing numerous challenges such as regulatory compliance, collision avoidance, and real-world operational feasibility.\n",
    "\n",
    "Current research on ASVs has made significant strides in integrating reinforcement learning (RL) techniques to develop autonomous navigation systems that can react to dynamic maritime environments. However, achieving full compliance with the International Regulations for Preventing Collisions at Sea (COLREGs) remains a critical hurdle. Most RL-based approaches focus on collision avoidance but often lack explicit mechanisms for adhering to navigation rules, especially in complex multi-vessel scenarios. This research aims to partially implement COLREG-compliant behavior in an RL-trained ASV, focusing specifically on key collision avoidance rules.\n",
    "\n",
    "### 1.2 The Role of the Human Factor in Marine Navigation\n",
    "Traditional marine navigation relies heavily on human judgment and experience to assess and mitigate collision risks. Despite technological advancements such as radar, AIS (Automatic Identification System), and electronic chart systems, human errors remain a leading cause of maritime accidents. Fatigue, misinterpretation of navigational data, and delayed decision-making contribute significantly to collision incidents. Autonomous navigation systems have the potential to mitigate these risks by ensuring more consistent, data-driven decision-making. However, for such systems to be viable, they must adhere to the same legal and operational standards as human-operated vessels.\n",
    "\n",
    "Ensuring compliance with COLREGs is a crucial aspect of integrating ASVs into real-world maritime traffic. While human mariners rely on experience and situational awareness to make decisions, an RL-based ASV must learn collision avoidance through a structured reward system. This study explores how an ASV can navigate toward a waypoint while avoiding collisions by learning COLREG-compliant maneuvers without explicit waypoint-tracking algorithms.\n",
    "\n",
    "### 1.3 Definition and Key COLREG Rules\n",
    "The International Regulations for Preventing Collisions at Sea (COLREGs) are the globally accepted rules that govern vessel movements to prevent collisions. These rules apply to all vessels at sea and dictate how ships should interact in different navigation scenarios. The following COLREG rules are particularly relevant to this study:\n",
    "\n",
    "**Rule 14: Head-on Situation**\n",
    "When two power-driven vessels are approaching on reciprocal or nearly reciprocal courses, both vessels must alter course to starboard to ensure they pass on each other’s port side.\n",
    "\n",
    "**Rule 15: Crossing Situation**\n",
    "When two power-driven vessels are crossing and there is a risk of collision, the vessel that has the other on its starboard side must keep out of the way and, if possible, avoid crossing ahead of the other vessel.\n",
    "\n",
    "**Rule 16: Action by the Give-Way Vessel**\n",
    "Any vessel required to keep out of the way of another vessel must take early and substantial action to stay well clear.\n",
    "\n",
    "**Rule 6: Safe Speed**\n",
    "Every vessel must proceed at a speed that allows it to take proper and effective action to avoid collisions, considering the prevailing circumstances and conditions.\n",
    "\n",
    "**Rule 8: Action to Avoid Collision:**\n",
    "\n",
    "- (b) Course or speed alterations must be substantial enough to be readily apparent to other vessels. A series of small alterations should be avoided.\n",
    "- (c) If there is enough sea room, altering course alone may be the most effective way to avoid a close-quarters situation, provided it is done in good time and does not create another hazard.\n",
    "- (d) The action taken must ensure a safe passing distance and should be continuously monitored until the vessels are past and clear.\n",
    "- (e) If necessary, a vessel should reduce speed or stop to assess the situation better and avoid collision.\n",
    "\n",
    "\n",
    "The following COLREG rules are omitted in this research:\n",
    "\n",
    "**Rule 17: Action by the Stand-on Vessel** – The responsibilities of the vessel that has the right of way are not considered in this study.\n",
    "\n",
    "**Rule 13: Overtaking** – Situations involving overtaking maneuvers are not included due to the complexity of modeling them within the given scope.\n",
    "\n",
    "For the purpose of the study, own ship is **ALWAYS GIVE-WAY** vessel and she must act according to the rules.\n",
    "\n",
    "### 1.4 Assumptions and Limitations\n",
    "This study makes several simplifying assumptions to focus on reinforcement learning-based collision avoidance without additional complexity:\n",
    "\n",
    "- Ship dynamics are ignored – The ASV's maneuvering characteristics, such as acceleration, turning radius, and inertia, are not explicitly modeled. The vessel is assumed to change course and speed instantly without delay.\n",
    "- Environmental dynamics are ignored – External factors like wind, currents, and waves are not considered in the simulation. The focus is solely on collision avoidance in an idealized static environment.\n",
    "- Target dynamics are stripped to basics - maintaining course and speed.\n",
    "- Limited scope due to time and complexity – Implementing full COLREG compliance in an RL-based agent is highly complex and time-consuming. Therefore, this research prioritizes the fundamental rules of collision avoidance and waypoint navigation.\n",
    "\n",
    "\n",
    "By defining these constraints, this study aims to analyze how reinforcement learning alone, without traditional waypoint-tracking algorithms or cross-track error corrections, can develop COLREG-compliant behavior in an ASV. The results provide a foundation for future improvements and real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a45dc-ab4e-44e1-b7e9-9ca15ad4dce1",
   "metadata": {},
   "source": [
    "## 2. Previous Research\n",
    "Several research studies have explored the integration of reinforcement learning (RL) into autonomous surface vehicles (ASVs) to ensure compliance with the International Regulations for Preventing Collisions at Sea (COLREGs). Notable contributions include:\n",
    "\n",
    "**[Risk-Based Implementation of COLREGs Using Deep Reinforcement Learning](https://arxiv.org/abs/2112.00115)**: This study presents a method that incorporates a subset of COLREGs into a deep RL-based path-following and obstacle avoidance system. The approach dynamically balances path following and COLREG-compliant collision avoidance in various scenarios, including training environments and real-world simulations.\n",
    "\n",
    "**[COLREGs-Compliant Autonomous Collision Avoidance Method](https://arxiv.org/abs/2006.09540)**: This research introduces an algorithm that enables unmanned surface vehicles (USVs) to navigate safely to target points without collisions. The proposed method ensures that decisions made by the USV adhere to COLREGs, enhancing maritime safety. \n",
    "\n",
    "**[Collision Avoidance for Autonomous Ships Using Deep Reinforcement Learning](https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2022.1084763/full)**: The study develops an intelligent collision avoidance algorithm based on approximate representation reinforcement learning (AR-RL). By combining prior knowledge with COLREGs, the model optimizes performance in dynamic environments, facilitating safe navigation for maritime autonomous surface ships (MASS). \n",
    "\n",
    "**[COLREGs-Based Path Planning for USVs Using Deep Reinforcement Learning](https://www.mdpi.com/2077-1312/11/12/2334)**: This research proposes a two-stage deep reinforcement learning approach for cooperative path planning of unmanned surface vehicles. The method addresses collision avoidance while adhering to COLREGs and considers both intra-fleet and external vessel interactions. \n",
    "\n",
    "**[A Novel Reinforcement Learning Collision Avoidance Algorithm for USVs](https://www.researchgate.net/publication/359110601_A_Novel_Reinforcement_Learning_Collision_Avoidance_Algorithm_for_USVs_Based_on_Maneuvering_Characteristics_and_COLREGs)**: This paper presents an autonomous collision avoidance algorithm that complies with USV maneuverability and COLREGs. The reinforcement learning agent learns collision avoidance maneuvers without prior human knowledge, utilizing a double-DQN method to reduce overestimation of the action-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eac587-131c-429a-a814-d9f029daf28c",
   "metadata": {},
   "source": [
    "## 3. Environment\n",
    "The simulation environment is implemented using Gymnasium, a framework widely used for reinforcement learning (RL). The MarineEnv class inherits from gym.Env, making it compatible with reinforcement learning libraries such as Stable-Baselines3. By extending gym.Env, the environment adheres to the standard structure required for RL training, including predefined methods such as `reset(), step(), render(), and close()`. This allows seamless interaction with RL agents, which learn optimal navigation strategies through trial and error.\n",
    "\n",
    "### 3.1 Observation Space\n",
    "The observation space in this environment is continuous, meaning it consists of real-valued numerical inputs that describe the agent’s current state. The observation space provides information on the own ship, next waypoint and surrounding targets , allowing the agent to make informed navigation decisions.\n",
    "\n",
    "**Own Ship State:** The ASV's state includes its course, speed, distance to the waypoint (WP), estimated time of arrival (ETA), and relative bearing to the WP. This data helps the agent assess progress toward the navigation goal.\n",
    "\n",
    "**Target Ships State:** Each detected target ship is represented by its bearing, closest point of approach (CPA), time to CPA (TCPA), course, speed, and distance. These parameters are crucial for collision avoidance.\n",
    "\n",
    "\n",
    "The observation space is structured using a method called `_define_observation_space()`, which ensures consistency and scalability when adding more features or refining the environment. The environment supports multiple targets, each represented within a tuple of dictionaries. Targets are dynamically assigned one of three possible encounter situations: crossing, head-on, or static.\n",
    "\n",
    "The geographical bounds of the environment are computed based on the `ENV_RANGE` parameter, which determines the navigable area in nautical miles. The ASV starts at `(0.0, 0.0)`, and all movements are calculated relative to this initial reference point.\n",
    "\n",
    "Scaling of the environment is set using `timescale` parameter with a default value of 1, meaning each step in the environment equals 1 minute of real-world time.\n",
    "\n",
    "### 3.2 Action Space\n",
    "The environment employs a continuous action space, allowing the RL agent to make fine-grained adjustments to the ASV’s movement. The agent controls two variables:\n",
    "\n",
    "**Course Change:** The agent can adjust the ship’s heading within a range of `-1` (maximum turn to port) to `1` (maximum turn to starboard). The maximum turning rate is 20 degrees per minute, meaning a full `1` value corresponds to a 20-degree starboard turn.\n",
    "\n",
    "**Speed Change:** The agent can accelerate or decelerate within a range of `-1` to `1`, where `1` corresponds to an increase of 0.5 knots per minute and `-1` results in a decrease of 0.5 knots per minute.\n",
    "\n",
    "Each simulation step represents one minute of real-world time. This means that a continuous change in course and speed accumulates over multiple time steps, reflecting realistic ship maneuvering constraints.\n",
    "\n",
    "During training, the environment operates with a timescale of 1/3, meaning each step represents 20 seconds of real-world time. All course and speed adjustments, as well as position updates, are scaled accordingly to reflect this timescale.\n",
    "\n",
    "### 3.3 Reset Method\n",
    "The `reset()` method initializes the environment at the start of an episode. It sets up the own ship, assigns a waypoint, and places target ships in realistic maritime scenarios.\n",
    "\n",
    "#### 3.3.1 Waypoint Setup\n",
    "The waypoint (WP) serves as the navigation goal for the ASV. It is randomly placed within a predefined range from the initial position to ensure variability across episodes. The WP’s distance is constrained to be within a practical navigation range, ensuring that the ASV has enough maneuvering room to encounter target ships along the way.\n",
    "\n",
    "When the WP is placed, the ASV calculates:\n",
    "\n",
    "- Distance to WP – How far the ship must travel to reach the target.\n",
    "- ETA to WP – The estimated arrival time based on current speed.\n",
    "- Relative Bearing to WP – The directional offset between the ASV’s current heading and the WP’s position.\n",
    "\n",
    "#### 3.3.2 Target Setup\n",
    "Target ships are introduced into the environment based on different encounter scenarios:\n",
    "\n",
    "- Crossing Situation: A target approaches from the starboard side, requiring the ASV to act as a give-way vessel by altering to starboard or/and reducing speed.\n",
    "- Head-on Situation: The ASV and target vessel are moving toward each other on reciprocal courses, necessitating a starboard turn per COLREG Rule 14.\n",
    "- Static Target: A stationary obstacle that the ASV must avoid.\n",
    "\n",
    "Each target is assigned a course, speed, and initial position based on real-world maritime navigation principles. The placement ensures that at least one target represents a collision risk during the episode.\n",
    "\n",
    "### 3.4 Step Method\n",
    "The `step()` method simulates the own ship’s movement and updates the state of detected targets. The RL agent selects an action, which affects the ASV’s course and speed.\n",
    "\n",
    "#### 3.4.1 Own Ship and Target Movement\n",
    "The own ship updates its course and speed based on the selected action.\n",
    "Detected target ships move according to their assigned speed and course, dynamically updating their CPA and TCPA relative to the ASV.\n",
    "If a target moves beyond the simulation bounds, it is removed from the active target list.\n",
    "#### 3.4.2 Reward Function\n",
    "The reward function evaluates the agent’s actions based on two primary objectives:\n",
    "\n",
    "**Waypoint Tracking**: If no immediate collision threats are detected, the agent receives rewards based on progress toward the waypoint. Positive rewards are given for:\n",
    "\n",
    "- Reducing the distance to the WP.\n",
    "- Maintaining alignment with the WP’s bearing (ASV stays on track).\n",
    "- Maintaining ETA to the WP (No random speed changes), \n",
    "\n",
    "**COLREG-Compliant Collision Avoidance**: If a target is classified as dangerous (i.e., it falls within CPA and TCPA limits), COLREG rules take precedence over waypoint tracking. During this phase:\n",
    "\n",
    "- Waypoint tracking is put on hold until the ASV successfully avoids the dangerous target.\n",
    "- The agent is rewarded for correct COLREG-compliant maneuvers, such as making a starboard turn in a head-on encounter or giving way in a crossing situation.\n",
    "- Penalties are applied for incorrect actions, such as violating Rule 14 (head-on) or making unnecessary maneuvers.\n",
    "- A collision penalty is applied if the ASV fails to avoid an obstacle. If the ASV successfully passes a target at a safe CPA distance, it resumes WP tracking and continues navigation toward its goal.\n",
    "\n",
    "### 3.5 Ship Classes and Methods\n",
    "The simulation includes three types of vessels:\n",
    "\n",
    "#### 3.5.1 Base Ship\n",
    "The BaseShip class defines core properties and movement methods applicable to all vessels:\n",
    "\n",
    "- Position (latitude, longitude)\n",
    "- Course (heading in degrees)\n",
    "- Speed (knots)\n",
    "\n",
    "Basic navigation methods for calculating bearing, distance, and speed adjustments.\n",
    "\n",
    "#### 3.5.2 Own Ship\n",
    "The OwnShip class extends BaseShip and introduces:\n",
    "\n",
    "- Target Detection: Identifies nearby vessels and classifies them as dangerous or safe.\n",
    "- Relative Navigation: Calculates CPA and TCPA for collision avoidance.\n",
    "\n",
    "The class incorporates methods for target/wp tracking.\n",
    "\n",
    "#### 3.5.3 Target Ship\n",
    "The Target class extends BaseShip and includes:\n",
    "\n",
    "- Relative Bearing and Distance to the ASV\n",
    "- Aspect Classification (Crossing, Head-on, or Static)\n",
    "- Danger Status (Determines if collision avoidance should be applied)\n",
    "\n",
    "### 3.6 Video Rendering and Visualization\n",
    "The environment includes real-time rendering using Pygame, allowing for visual monitoring of the agent’s training process. The graphical representation follows a structured color scheme to distinguish between different vessels, waypoints, and motion vectors.\n",
    "\n",
    "#### 3.6.1 Visualization Elements\n",
    "- White dot → Represents the own ship (ASV).\n",
    "- Green dot → Represents the waypoint (WP), the ASV’s navigation goal.\n",
    "- Blue dots → Represent the target ships, which may be moving or static obstacles.\n",
    "#### 3.6.2 Motion Vectors\n",
    "To provide an intuitive representation of ship movements:\n",
    "\n",
    "- Red line → Represents the own ship’s heading vector, with its length scaled to the ASV’s speed in knots.\n",
    "- Blue line → Represents the heading vector of target ships, similarly scaled to target speed.\n",
    "#### 3.6.3 Tracking and CPA Visualization\n",
    "- Purple ring → Displays the Closest Point of Approach (CPA) limit set for collision avoidance (default: 1 NM).\n",
    "- White track line → Shows the historical path (trace) of the own ship, allowing for trajectory analysis.\n",
    "- Blue track lines → Represent the path history of target ships, illustrating their movement over time.\n",
    "#### 3.6.4 Rendering in Training and Evaluation\n",
    "During training, rendering is optional and mainly used for debugging.\n",
    "\n",
    "In evaluation runs, rendering helps assess the agent’s COLREG compliance, decision-making, and waypoint tracking performance.\n",
    "This visualization provides an intuitive representation of maritime navigation within the RL simulation, making it easier to analyze how the ASV interacts with waypoints and target vessels in different scenarios.\n",
    "\n",
    "By integrating these components, the simulation environment provides a realistic and structured testbed for training an RL-based ASV, ensuring that it navigates effectively while complying with maritime collision regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725b0f8-3272-4795-b426-4d359686a353",
   "metadata": {},
   "source": [
    "## 4. Model Training and Selection\n",
    "The training process for the COLREG-compliant ASV is structured into several stages, involving model selection, hyperparameter tuning, and evaluation. The goal is to develop an RL agent capable of waypoint navigation and collision avoidance, ensuring compliance with COLREGs.\n",
    "\n",
    "### 4.1 Model Selection: Why DRL? Why PPO?\n",
    "\n",
    "#### 4.1.1 Why Reinforcement Learning Instead of Standard Machine Learning?\n",
    "Traditional supervised learning approaches in machine learning rely on large labeled datasets to map inputs to outputs. However, maritime navigation is an inherently sequential decision-making problem, where an agent must continuously adjust its actions based on dynamic environmental conditions.\n",
    "\n",
    "Using standard machine learning techniques for ship navigation would be impractical due to:\n",
    "\n",
    "- Lack of labeled data: The model would require manually labeled examples of every possible scenario, which is infeasible for complex maritime interactions.\n",
    "- Sequential dependencies: Maritime navigation involves long-term decision-making—an action taken now affects the future state of the vessel.\n",
    "- Exploration-exploitation tradeoff: Standard ML models do not explore alternative strategies, whereas reinforcement learning (RL) allows an agent to discover optimal maneuvers over time.\n",
    "\n",
    "Deep RL, enables an Autonomous Surface Vehicle (ASV) to learn by interacting with its environment and improving its policy iteratively. Deep RL is more flexible than traditional ML because it:\n",
    "\n",
    "- Learns optimal policies without explicit supervision.\n",
    "- Adapts to dynamic environments (e.g., moving targets, changing encounter scenarios).\n",
    "- Can handle continuous control problems, such as adjusting course and speed in real-time.\n",
    "\n",
    "Thus, deep reinforcement learning is a natural fit for COLREG-compliant ASV navigation, as it balances waypoint tracking and collision avoidance dynamically.\n",
    "\n",
    "#### 4.1.2 Why Proximal Policy Optimization (PPO)?\n",
    "PPO was chosen as the reinforcement learning framework for this project due to its stability, sample efficiency, and suitability for continuous control tasks. PPO falls under Policy Gradient Methods, where the policy (agent's behavior) is directly optimized.\n",
    "\n",
    "Key Advantages of PPO for Maritime Navigation\n",
    "- Stability and Sample Efficiency\n",
    "\n",
    "PPO introduces a clipped objective function, which prevents large, unstable policy updates.\n",
    "This improves training stability, avoiding drastic behavior shifts that could lead to unsafe ASV maneuvers.\n",
    "Unlike traditional Policy Gradient methods, PPO reduces variance without requiring second-order derivatives (like Trust Region Policy Optimization, TRPO). Not too sensitive to hyperparameters tunning (as seen from the optimization results)\n",
    "- Continuous Action Space Handling\n",
    "\n",
    "ASVs require precise adjustments in heading and speed to comply with COLREG rules.\n",
    "PPO operates well in continuous action spaces, where minor corrections are crucial for safe navigation.\n",
    "Other RL methods (e.g., DQN) struggle with continuous control problems.\n",
    "- Scalability for Parallel Training\n",
    "\n",
    "PPO allows training across multiple parallel environments (n_envs=8 in this setup).\n",
    "This accelerates learning by gathering diverse experiences in different maritime scenarios.\n",
    "More training data leads to faster convergence.\n",
    "- Robustness in Dynamic Environments\n",
    "\n",
    "The maritime environment is highly dynamic, with moving target vessels.\n",
    "PPO adapts well to changing conditions, learning from a variety of COLREG-compliant encounters.\n",
    "The policy-based approach enables PPO to generalize to new situations better than pure value-based methods.\n",
    "\n",
    "In this project, PPO with Clipped Surrogate Objective is used. This means the model optimizes the objective function by taking the minimum of the unclipped objective and the clipped surrogate objective, preventing excessively large policy updates to improve stability. \n",
    "\n",
    "The clipped loss function is the expected value over time steps of the minimum between two terms: first, the probability ratio, which measures the change in policy probability between the new and old policy, multiplied by the advantage estimate, which indicates how much better an action is compared to the expected return. Second, the clipped probability ratio, which restricts the change within a specified range to prevent overly large policy updates, also multiplied by the advantage estimate.\n",
    "\n",
    "$$\n",
    "L_{\\text{CLIP}}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\, \\text{clip} \\left( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\right]\n",
    "$$\n",
    "where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$ is the ratio between the new policy and the old policy\n",
    "- $\\hat{A}_t$ is the advantage estimation\n",
    "- $\\hat{\\mathbb{E}}_t$ is the expectation over timesteps,\n",
    "- $\\epsilon$ is the clip coefficient\n",
    "\n",
    "The final loss function in Proximal Policy Optimization (PPO) is a weighted combination of three components - policy loss, value loss and entropy loss:\n",
    "\n",
    "$$\n",
    "L_{\\text{CLIP+VF+S}}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ L_{\\text{CLIP},t}(\\theta) - c_1 L_{\\text{VF},t}(\\theta) + c_2 S[\\pi_{\\theta}](s_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $L_{\\text{CLIP+VF+S}}(\\theta)$ is the combined total loss,\n",
    "- $L_{\\text{CLIP},t}(\\theta)$ is the clipped policy loss,\n",
    "- $c_1 L_{\\text{VF},t}(\\theta)$ is the value function loss, multiplied by a weighting coefficient $c_1$,\n",
    "- $c_2 S[\\pi_{\\theta}](s_t)$ is the entropy loss, multiplied by a weighting coefficient $c_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d73809d-8c41-48f9-a0d0-443d2c4eb3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=36, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=36, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create folders for logs and videos\n",
    "log_folder = './project_logs'\n",
    "video_folder = os.path.join(log_folder, 'video')\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "PPO.load(os.path.join(log_folder, 'optuna/best_model/best_model.zip'), device='cpu').policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cf6b3-5a10-46f5-a62f-c1ca6c7e2c2f",
   "metadata": {},
   "source": [
    "#### 4.1.3 Policy Architecture Overview\n",
    "The Actor-Critic Policy used in the PPO agent consists of several key components responsible for feature extraction, policy learning, and value estimation. Below is an overview of the architecture:\n",
    "\n",
    "**Feature Extractors**\n",
    "\n",
    "The model includes three FlattenExtractor modules:\n",
    "\n",
    "- General feature extractor for processing raw observations.\n",
    "- Policy feature extractor $\\pi$ to transform state inputs for action selection.\n",
    "- Value function feature extractor $\\text{V}$ to estimate state values for advantage computation.\n",
    "These extractors flatten input tensors to make them suitable for processing by the Multi-Layer Perceptron (MLP) network.\n",
    "\n",
    "**MLP Feature Extractor**\n",
    "\n",
    "The core of the network is the MlpExtractor, which consists of two independent networks:\n",
    "\n",
    "- Policy Network: Maps input features to action probabilities.\n",
    "- Value Network: Maps input features to a scalar value representing expected returns from a given state.\n",
    "\n",
    "Each network has:\n",
    "\n",
    "- Two fully connected layers with 64 neurons each.\n",
    "- Tanh activation functions ensuring smooth, bounded outputs.\n",
    "\n",
    "**Output Layers**\n",
    "\n",
    "- Action Network: A linear layer mapping the last policy layer (64 features) to 2 output neurons, representing the action space.\n",
    "- Value Network: A linear layer mapping the last value layer (64 features) to a single scalar, estimating the value of a given state.\n",
    "\n",
    "This architecture ensures that the actor (policy network) and critic (value network) share representations while maintaining separate pathways for action selection and value estimation, following the actor-critic reinforcement learning paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34470c3e-0614-40c9-9c2e-debaabf1ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-292a91e750d14f1a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-292a91e750d14f1a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir {log_folder} --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6482d68a-fce7-40e6-8d5a-275862ba21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'MarineEnv-v0' not in registry:\n",
    "    register(\n",
    "        id='MarineEnv-v0',\n",
    "        entry_point='environments:MarineEnv',  # String reference to the class\n",
    "    )\n",
    "\n",
    "def yield_random_seed():\n",
    "    while True:\n",
    "        yield np.random.randint(low=1, high=201)\n",
    "seed_generator = yield_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab763a-5e81-4c36-8dbe-46d85dee9c72",
   "metadata": {},
   "source": [
    "### 4.2 Training Process\n",
    "\n",
    "Tensorboard was used to visualize the learning progress of the agent.\n",
    "\n",
    "#### 4.2.1 Initial Training with Default Hyperparameters\n",
    "The training process began using default PPO hyperparameters to establish a baseline performance. At this stage, the RL agent was trained in a controlled environment where the ASV learned to:\n",
    "\n",
    "- Move toward a waypoint.\n",
    "- Avoid static obstacles.\n",
    "- Perform simple navigation maneuvers.\n",
    "\n",
    "The default parameters used in initial training included:\n",
    "\n",
    "- gamma = 0.99 (discount factor for long-term rewards)\n",
    "- clip_range = 0.2 (limits the size of policy updates)\n",
    "- n_steps = 2048 (number of steps before updating the policy)\n",
    "- batch_size = 64 (batch size for training)\n",
    "- gae_lambda = 0.95 (Generalized Advantage Estimation smoothing)\n",
    "\n",
    "This initial phase provided insight into the agent’s basic behavior and highlighted areas that required improvement.\n",
    "\n",
    "#### 4.2.2 Hyperparameter Optimization with Optuna\n",
    "To improve performance, an extensive hyperparameter tuning process was conducted using Optuna, an optimization framework that automates the search for optimal hyperparameters. The key objectives were:\n",
    "\n",
    "- Improving COLREG compliance: Ensuring that the ASV consistently follows maritime collision avoidance rules.\n",
    "- Balancing waypoint tracking and collision avoidance: Ensuring that the ASV prioritizes waypoint tracking when no immediate collision risk is present.\n",
    "- Optimizing learning stability and efficiency: Preventing issues like premature convergence or overly cautious navigation.\n",
    "\n",
    "The optimization of the model will be further explained in Section 5 and new model will be trained with best hyperparameters according to the Optuna study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c63fc0-09d2-4f9c-98a3-6f4539521a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment kwargs\n",
    "timescale = 1 / 3\n",
    "env_kwargs = dict(\n",
    "    render_mode='rgb_array',\n",
    "    continuous=True,\n",
    "    max_episode_steps=int(400 / timescale),\n",
    "    training_stage=2,\n",
    "    timescale=timescale,\n",
    "    training=True,\n",
    "    total_targets=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50503b85-dfaa-49bf-8524-8bf1e3f00722",
   "metadata": {},
   "source": [
    "### 4.3 Training Execution\n",
    "#### 4.3.1 Multi-Environment Training Setup\n",
    "The agent was trained using 8 parallel environments (n_envs=8) to accelerate learning. This allowed the model to gather experience faster, reducing training time while improving generalization.\n",
    "\n",
    "A separate evaluation environment was created to periodically assess the agent’s performance without affecting training. This evaluation was performed every 5000 steps, and the best-performing model was saved automatically.\n",
    "\n",
    "Video recording was performed during training. Videofiles located at `./project_logs/videos/training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceee0548-ec75-47e1-afc1-09c26ce4ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup of the training envs\n",
    "train_env = make_vec_env(env_id='MarineEnv-v0', n_envs=8, env_kwargs=env_kwargs)\n",
    "eval_env = gym.make('MarineEnv-v0', **env_kwargs)\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# video recording of the training process\n",
    "# eval_env = RecordVideo(eval_env, video_folder, episode_trigger=lambda x: x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55081ef7-3915-4f60-97b9-d38f9d3f0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval callback - used to track the statistics of the training\n",
    "eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=os.path.join(log_folder, 'default/best_model/'),\n",
    "        log_path=os.path.join(log_folder, 'default/results/'),\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3375503c-fcdc-4603-9365-e07a4e7bf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup of the defaut kwargs for initial training of PPO agent\n",
    "default_kwargs = {\n",
    "    'learning_rate': 3e-4,\n",
    "    'n_steps': 2048,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 10,\n",
    "    'gamma': 0.99, \n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_range': 0.2,\n",
    "    'ent_coef': 0.0,\n",
    "    'vf_coef': 0.5, \n",
    "    'max_grad_norm': 0.5,\n",
    "    'target_kl': None,\n",
    "    'tensorboard_log': log_folder,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b198ce-f766-4415-8955-bf9a598b71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish initial agent\n",
    "agent = PPO(\n",
    "        policy='MlpPolicy',\n",
    "        env=train_env,\n",
    "        verbose=0,\n",
    "        device='cpu',\n",
    "        **default_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c44e0-b6d0-4f45-b64d-af4b1f83a6e8",
   "metadata": {},
   "source": [
    "The agent was then trained for 600,000 timesteps with a progress bar for real-time monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fb630c-1336-43b2-907f-0e9b80daa907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# agent.learn(total_timesteps=int(6e5), reset_num_timesteps=True, tb_log_name='default/default_best', progress_bar=True, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833581f3-5b2e-468a-a6be-10a6075abdf2",
   "metadata": {},
   "source": [
    "#### 4.3.2 Model Evaluation\n",
    "After training, the model was evaluated using 10 test episodes, where it was deployed in a separate environment `(eval_env)` with identical parameters.\n",
    "\n",
    "The performance was measured using mean and standard deviation of rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466d533e-0a5d-4c2d-aaf5-467bad256d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 564.86, Std: 384.15\n"
     ]
    }
   ],
   "source": [
    "default_best_agent = PPO.load(os.path.join(log_folder, 'default/best_model/best_model.zip'), device='cpu')\n",
    "eval_env = Monitor(gym.make('MarineEnv-v0', **env_kwargs))\n",
    "mean, std = evaluate_policy(model=default_best_agent, env=eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'Mean: {mean:.2f}, Std: {std:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025191b-bcf5-4053-b3fa-a44611c65349",
   "metadata": {},
   "source": [
    "### 4.4 Model Deployment and Visualization\n",
    "#### 4.4.1 Testing the Trained Model\n",
    "The trained agent was deployed in a human-rendered environment (render_mode='human'), where it performed real-time decision-making in a scenario with 3 dynamic target ships.\n",
    "\n",
    "Each episode ran for up to 400 time steps, simulating more than 50 minutes of real-world navigation per episode.\n",
    "\n",
    "During each test episode, ASV’s step-by-step reward accumulation was logged. This real-world simulation provided qualitative validation of the model's COLREG compliance and maneuvering decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1768e6-1c27-4e5b-9ffc-66d9743dad97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_video_rendering(\n",
    "    agent: Any, \n",
    "    episodes: int = 3, \n",
    "    timescale: float = 1/6, \n",
    "    seed: Optional[int] = None, \n",
    "    record_video: bool = False, \n",
    "    episode_trigger: Optional[Callable[[int], bool]] = None, \n",
    "    name_prefix: Optional[str] = None, \n",
    "    video_folder: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Runs a simulation of the MarineEnv-v0 environment using the given agent, with optional video recording.\n",
    "\n",
    "    Parameters:\n",
    "    - agent: The trained agent used for inference.\n",
    "    - episodes (int): Number of episodes to run (default is 3).\n",
    "    - timescale (float): The simulation timescale factor (default is 1/6).\n",
    "    - seed (int, optional): Random seed for environment initialization. If None, a generated seed is used.\n",
    "    - record_video (bool): If True, records the simulation as a video (default is False).\n",
    "    - episode_trigger (function, optional): A function that determines which episodes get recorded.\n",
    "    - name_prefix (str, optional): Prefix for recorded video file names.\n",
    "    - video_folder (str, optional): Path to save recorded videos.\n",
    "\n",
    "    Behavior:\n",
    "    - If video recording is enabled, the environment is wrapped with RecordVideo.\n",
    "    - Logs episode statistics, including total rewards, episode length, and termination status.\n",
    "    - If recording, logs are saved to a text file in the specified video folder.\n",
    "    - Displays simulation results in the console if video recording is disabled.\n",
    "\n",
    "    Returns:\n",
    "    - None. The function either logs the results to a file or prints them to the console.\n",
    "    \"\"\"\n",
    "    \n",
    "    if seed is None:\n",
    "        seed = next(seed_generator)\n",
    "    \n",
    "    env = gym.make(\n",
    "        'MarineEnv-v0',\n",
    "        render_mode='rgb_array' if record_video else 'human',\n",
    "        continuous=True,\n",
    "        training_stage=2,\n",
    "        timescale=timescale,\n",
    "        training=False,\n",
    "        total_targets=3,\n",
    "        seed=seed,\n",
    "    )\n",
    "    \n",
    "    if record_video:\n",
    "        from IPython.display import HTML\n",
    "        from base64 import b64encode\n",
    "        \n",
    "        if episode_trigger is None:\n",
    "            episode_trigger = lambda episode_id: True\n",
    "\n",
    "        if video_folder is None:\n",
    "            video_folder = video_folder   \n",
    "        else:\n",
    "            if name_prefix:\n",
    "                video_folder = os.path.join(video_folder, name_prefix)\n",
    "        \n",
    "        # wrap environment for video recording\n",
    "        env = RecordVideo(\n",
    "            env=env, \n",
    "            video_folder=video_folder, \n",
    "            episode_trigger=lambda episode_id: True, \n",
    "            name_prefix=name_prefix)\n",
    "        \n",
    "    logged_episodes = []\n",
    "    logged_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_rewards = 0\n",
    "        eta = state[5]\n",
    "        \n",
    "        for step in range(int(400 / timescale)):\n",
    "            with torch.no_grad():\n",
    "                action = agent.predict(state, deterministic=True)\n",
    "            \n",
    "            state, reward, terminated, truncated, info = env.step(action[0])\n",
    "            \n",
    "            if not record_video:\n",
    "                env.render()\n",
    "                \n",
    "            episode_rewards += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            time.sleep(0.005)\n",
    "         \n",
    "        result_string = f'Episode: {episode}\\nEpisode length: {step}, Elapsed real time: {round(step * timescale)} minutes, Initial WP ETA: {round(eta)} minutes\\nEpisode total rewards: {episode_rewards :.2f}\\nIs terminated: {info[\"terminated\"]}, Is truncated: {info[\"truncated\"]}\\n============================\\n'\n",
    "\n",
    "        logged_episodes.append(result_string)\n",
    "        logged_rewards.append(episode_rewards)\n",
    "\n",
    "    evaluation = f'Mean: {np.array(logged_rewards).mean():.2f}, Std: {np.array(logged_rewards).std():.2f}, Initial seed: {seed}'\n",
    "    logged_episodes.append(evaluation)\n",
    "    # Open log file\n",
    "    if record_video:\n",
    "        log_file_path = os.path.join(video_folder, name_prefix + '.txt')\n",
    "        with open(log_file_path, 'w') as log_file:\n",
    "            log_file.write('\\n'.join(logged_episodes))\n",
    "            print(f\"Training log saved at: {log_file_path}\")\n",
    "    else:\n",
    "        print('\\n'.join(logged_episodes))\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa8c86a-1dac-4276-a9cb-1c37e9e8cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 152\n",
      "Episode: 0\n",
      "Episode length: 302, Elapsed real time: 50 minutes, Initial WP ETA: 48 minutes\n",
      "Episode total rewards: 368.58\n",
      "Is terminated: WP Reached!, Is truncated: False\n",
      "============================\n",
      "\n",
      "Episode: 1\n",
      "Episode length: 360, Elapsed real time: 60 minutes, Initial WP ETA: 59 minutes\n",
      "Episode total rewards: 754.62\n",
      "Is terminated: WP Reached!, Is truncated: False\n",
      "============================\n",
      "\n",
      "Episode: 2\n",
      "Episode length: 567, Elapsed real time: 94 minutes, Initial WP ETA: 97 minutes\n",
      "Episode total rewards: 741.98\n",
      "Is terminated: WP Reached!, Is truncated: False\n",
      "============================\n",
      "\n",
      "Mean: 621.72, Std: 179.07, Initial seed: 152\n"
     ]
    }
   ],
   "source": [
    "run_video_rendering(default_best_agent, episodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755325a4-8818-4273-a174-3261107df468",
   "metadata": {},
   "source": [
    "#### 4.4.2 Video Recording of Model Performance\n",
    "To visually inspect the ASV’s behavior, episodes were recorded using Gymnasium’s RecordVideo wrapper. The video files were automatically saved and later reviewed to analyze decision-making behavior.\n",
    "\n",
    "Multiple episodes were recorded to assess:\n",
    "\n",
    "- The agent’s reaction to head-on, crossing, and static targets.\n",
    "- Whether the ASV consistently made starboard turns in head-on encounters (Rule 14).\n",
    "- How well the ASV balanced COLREG compliance with efficient waypoint tracking.\n",
    "\n",
    "Recorded videos and episode statistics are provided in txt file, located:\n",
    "```bash\n",
    "./project_logs/video/defaut_agent/\n",
    "./project_logs/video/optimized_agent/\n",
    "```\n",
    "\n",
    "### 4.5 Summary\n",
    "The training process followed a structured pipeline:\n",
    "\n",
    "1. Model Selection – PPO was chosen for its stability and suitability for continuous control.\n",
    "2. Initial Training – Conducted with default PPO parameters to establish baseline performance.\n",
    "3. Hyperparameter Optimization – Used Optuna to fine-tune mainly learning rate, discount factor, clipping range, and entropy coefficient.\n",
    "4. Training Execution – Conducted using 8 parallel environments for efficiency, with regular evaluation checkpoints.\n",
    "5. Evaluation and Testing – The model was tested across multiple episodes to measure waypoint tracking and COLREG compliance.\n",
    "6. Deployment and Visualization – The trained model was deployed in a real-time simulation and video recordings were used for analysis.\n",
    "\n",
    "Through this approach, the ASV learned safe, COLREG-compliant navigation strategies purely through reinforcement learning, without explicit rule-based waypoint tracking or cross-track error corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97e249-cc60-4050-a692-122075255de2",
   "metadata": {},
   "source": [
    "## 5. Optimization\n",
    "The optimization process aimed to fine-tune PPO hyperparameters for optimal performance in training a COLREG-compliant ASV (Autonomous Surface Vehicle). The primary goal was to balance waypoint tracking and collision avoidance while ensuring stable training. This was achieved using Optuna, an automated hyperparameter tuning framework that efficiently searches for the best configuration.\n",
    "\n",
    "### 5.1 Optimization Strategy\n",
    "To systematically optimize the reinforcement learning model, the following strategy was employed:\n",
    "\n",
    "#### 5.1.1 Search Algorithm and Pruning Strategy\n",
    "- Sampler: Tree-structured Parzen Estimator (TPESampler) was chosen as the search algorithm, which efficiently selects promising hyperparameters based on past trials.\n",
    "- Pruner: A Median Pruner was used to terminate underperforming trials early, preventing unnecessary computations on bad configurations. This allowed the search process to focus computational resources on the most promising hyperparameter sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba742b-82cf-4b7d-b023-8f0d85bcbd12",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter Tuning Approach\n",
    "#### 5.2.1 Optimization Setup\n",
    "\n",
    "Following parameters used to run the study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e5bb2e-3548-47d2-9553-55d30f2c5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100  # max number of trials\n",
    "N_JOBS = 1 # number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 10  # number of evaluations during the training\n",
    "N_TIMESTEPS = int(2e5)  # training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 8 # number of the env for evaluation\n",
    "N_EVAL_EPISODES = 10\n",
    "# TIMEOUT = int(60 * 15)  # 15 minutes\n",
    "TIMEOUT = None # keep going till finish\n",
    "\n",
    "ENV_ID = 'MarineEnv-v0'\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1269a-31ba-4e3b-9224-9f8faaf115d4",
   "metadata": {},
   "source": [
    "The **evaluation metric** used was the **mean episodic reward**, which reflects how well the agent balances waypoint tracking and collision avoidance.\n",
    "\n",
    "#### 5.2.2 Hyperparameters Being Optimized\n",
    "Each trial used random sampling at the beginning (N_STARTUP_TRIALS), followed by Bayesian optimization via the TPE algorithm.\n",
    "\n",
    "Following the hyperparameters being optimized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe01608-7c9a-4e50-9cb5-f75f7b8af620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def sample_ppo_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Samples hyperparameters for Proximal Policy Optimization (PPO) using Optuna.\n",
    "\n",
    "    Parameters:\n",
    "    - trial (optuna.Trial): The Optuna trial object used for suggesting hyperparameter values.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: A dictionary containing the sampled hyperparameters for PPO.\n",
    "    \n",
    "    Sampled Hyperparameters:\n",
    "    - learning_rate (float): Learning rate for policy optimization, sampled logarithmically between 5e-5 and 1e-3.\n",
    "    - n_steps (int): Number of environment steps per update, sampled as a power of 2 between 1024 and 4096.\n",
    "    - batch_size (int): Batch size chosen from valid divisors of (n_steps * N_EVAL_ENVS).\n",
    "    - n_epochs (int): Number of PPO epochs per batch update, sampled between 8 and 12.\n",
    "    - gamma (float): Discount factor for future rewards, sampled between 0.95 and 0.9999.\n",
    "    - gae_lambda (float): Generalized Advantage Estimation (GAE) lambda, balancing bias-variance trade-off, sampled between 0.9 and 1.0.\n",
    "    - clip_range (float): Clipping range for PPO updates, sampled between 0.15 and 0.25.\n",
    "    - ent_coef (float): Entropy coefficient to encourage exploration, sampled logarithmically between 0.001 and 0.2.\n",
    "    - vf_coef (float): Value function loss coefficient, sampled between 0.3 and 0.7.\n",
    "    - max_grad_norm (float): Maximum gradient norm for clipping, sampled between 0.2 and 2.0.\n",
    "    - target_kl (float): KL divergence target for stopping updates, sampled between 0.02 and 0.15.\n",
    "    \n",
    "    Policy Architecture:\n",
    "    - activation_fn (torch.nn.Module): Activation function for neural network layers, chosen between Tanh and ReLU.\n",
    "    - net_arch (List[int]): Neural network architecture, set as [64, 64] for \"tiny\" and [128, 128] for \"small\".\n",
    "    \"\"\"\n",
    "        \n",
    "    learning_rate = trial.suggest_float('learning_rate', 5e-5, 1e-3, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int('n_steps', 10, 12)  # Number of steps per update\n",
    "    \n",
    "    valid_batch_sizes = [b for b in [64, 128, 256, 512, 1024] if (n_steps * N_EVAL_ENVS) % b == 0]\n",
    "    batch_size = trial.suggest_categorical('batch_size', valid_batch_sizes)\n",
    "    \n",
    "    n_epochs = trial.suggest_int('n_epochs', 8, 12)  # PPO update epochs per batch\n",
    "    gamma = trial.suggest_float('gamma', 0.95, 0.9999)  # Discount factor (close to 1 for long-term rewards)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.9, 1.0)  # GAE lambda (trade-off bias/variance)\n",
    "    clip_range = trial.suggest_float('clip_range', 0.15, 0.25)  # PPO clipping range\n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.001, 0.2, log=True)  # Entropy coefficient (for exploration)\n",
    "    vf_coef = trial.suggest_float('vf_coef', 0.3, 0.7)  # Value function loss coefficient\n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.2, 2.0)  # Gradient clipping\n",
    "    target_kl = trial.suggest_float('target_kl', 0.02, 0.15)  # KL divergence target\n",
    "    \n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "    net_arch = trial.suggest_categorical('net_arch', ['tiny', 'small'])\n",
    "    # Convert architecture choices\n",
    "    net_arch = [64, 64] if net_arch == 'tiny' else [128, 128]\n",
    "    activation_fn = {'tanh': nn.Tanh, 'relu': nn.ReLU}[activation_fn]\n",
    "    \n",
    "    return {\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_steps': n_steps,\n",
    "        'batch_size': batch_size,\n",
    "        'n_epochs': n_epochs,\n",
    "        'gamma': gamma, \n",
    "        'gae_lambda': gae_lambda,\n",
    "        'clip_range': clip_range,\n",
    "        'ent_coef': ent_coef,\n",
    "        'vf_coef': vf_coef, \n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'target_kl': target_kl,\n",
    "        'policy_kwargs': {\n",
    "            'net_arch': net_arch,\n",
    "            'activation_fn': activation_fn\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fce1a650-73a2-4e16-95ea-3646fb22976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    A custom Optuna evaluation callback for tracking and pruning PPO training trials.\n",
    "\n",
    "    This callback extends `EvalCallback` to integrate Optuna's pruning mechanism, allowing\n",
    "    for the early stopping of underperforming trials based on evaluation results.\n",
    "\n",
    "    Parameters:\n",
    "    - eval_env (gym.Env): The evaluation environment used to assess the agent's performance.\n",
    "    - trial (optuna.Trial): The Optuna trial object used for logging results and deciding pruning.\n",
    "    - n_eval_episodes (int, optional): Number of episodes to evaluate the policy. Default is 5.\n",
    "    - eval_freq (int, optional): Frequency (in environment steps) at which the policy is evaluated. Default is 10,000.\n",
    "    - deterministic (bool, optional): Whether to use a deterministic policy during evaluation. Default is True.\n",
    "    - verbose (int, optional): Verbosity level (0: silent, 1: info). Default is 0.\n",
    "\n",
    "    Attributes:\n",
    "    - trial (optuna.Trial): The associated Optuna trial for hyperparameter optimization.\n",
    "    - eval_idx (int): The current evaluation index, incremented after each evaluation.\n",
    "    - is_pruned (bool): Flag indicating whether the trial was pruned.\n",
    "\n",
    "    Methods:\n",
    "    - _on_step() -> bool: Overrides the parent method to evaluate the agent and report performance to Optuna.\n",
    "      If the trial performs poorly, it may be pruned to save computational resources.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if training should continue, False if the trial should be pruned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b3733ad-579f-4003-aa6f-0be7283c6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Defines the objective function for Optuna hyperparameter optimization of a PPO agent.\n",
    "\n",
    "    This function samples a set of hyperparameters, initializes the training and evaluation environments, \n",
    "    trains a PPO model, and returns the mean episodic reward as the evaluation metric.\n",
    "\n",
    "    Parameters:\n",
    "    - trial (optuna.Trial): The Optuna trial object used for sampling hyperparameters and logging results.\n",
    "\n",
    "    Returns:\n",
    "    - float: The mean episodic reward after training.\n",
    "\n",
    "    Raises:\n",
    "    - optuna.exceptions.TrialPruned: If the trial is pruned due to poor performance.\n",
    "\n",
    "    Workflow:\n",
    "    1. Copies default hyperparameters and updates them with sampled values.\n",
    "    2. Configures environment parameters and initializes training and evaluation environments.\n",
    "    3. Creates and trains the PPO model using the sampled hyperparameters.\n",
    "    4. Evaluates the model using the `TrialEvalCallback` and records performance.\n",
    "    5. Handles potential issues such as NaN values during training.\n",
    "    6. If the trial is pruned, it raises `TrialPruned()`, otherwise returns the last mean reward.\n",
    "\n",
    "    Notes:\n",
    "    - The function ensures proper environment closure to prevent memory leaks.\n",
    "    - If NaN values are encountered during training, the function returns NaN to indicate failure.\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    kwargs.update(**sample_ppo_params(trial))\n",
    "\n",
    "    # Ensure env_kwargs is passed correctly\n",
    "    env_kwargs = {\n",
    "        \"render_mode\": \"rgb_array\",\n",
    "        \"continuous\": True,\n",
    "        \"max_episode_steps\": 1200,\n",
    "        \"training_stage\": 2,\n",
    "        \"timescale\": 1/3,\n",
    "        \"total_targets\": 3,\n",
    "    }\n",
    "\n",
    "    # Create the training environment\n",
    "    train_env = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS, env_kwargs=env_kwargs)\n",
    "\n",
    "    # Create the RL model\n",
    "    model = PPO(\n",
    "        device='cpu', \n",
    "        verbose=0, \n",
    "        env=train_env, \n",
    "        tensorboard_log=os.path.join(log_folder, 'optimize'), \n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Create evaluation environment with same parameters\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS, env_kwargs=env_kwargs)\n",
    "\n",
    "    # Create callback for evaluation\n",
    "    eval_callback = TrialEvalCallback(eval_envs, trial, N_EVAL_EPISODES, EVAL_FREQ, deterministic=True, verbose=0)\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback, progress_bar=True)\n",
    "    except AssertionError as e:\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d1490f4-2501-4427-9855-530b14d9e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_best_parameters(best_trial_parameters: Dict, file_path: str = \"best_hyperparameters.json\") -> Dict:\n",
    "    \"\"\"\n",
    "    Exports the best hyperparameters from an Optuna trial to a JSON file for later use.\n",
    "\n",
    "    This function processes and formats the best trial parameters, ensuring compatibility for saving and reloading.\n",
    "    It modifies specific parameters such as the activation function, network architecture, and `n_steps` before \n",
    "    saving them to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - best_trial_parameters (Dict): The dictionary containing the best hyperparameters found by Optuna.\n",
    "    - file_path (str, optional): The filename for saving the best parameters. Default is \"best_hyperparameters.json\".\n",
    "\n",
    "    Returns:\n",
    "    - Dict: The processed dictionary of best hyperparameters, including formatted policy arguments.\n",
    "\n",
    "    Processing Steps:\n",
    "    - Converts the activation function from a PyTorch class to a string for JSON compatibility.\n",
    "    - Maps the network architecture from a string key to a corresponding list of layer sizes.\n",
    "    - Converts `n_steps` from an exponent value to its actual power-of-2 integer.\n",
    "    - Adds `policy_kwargs` containing the processed architecture and activation function.\n",
    "    - Includes the `tensorboard_log` directory for logging purposes.\n",
    "    - Saves the parameters in JSON format to the specified file path inside the log directory.\n",
    "\n",
    "    Notes:\n",
    "    - The function ensures that all parameters are JSON-serializable.\n",
    "    - The saved JSON file allows easy reloading of the best hyperparameters for future training.\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "        \n",
    "    # Convert activation function string to actual PyTorch class\n",
    "    activation_fn = best_trial_parameters.pop('activation_fn')\n",
    "    activation_fn = {'tanh': 'Tanh', 'relu': 'ReLU'}[activation_fn]  # Store as string for JSON compatibility\n",
    "\n",
    "    # Convert network architecture\n",
    "    net_arch = best_trial_parameters.pop('net_arch')\n",
    "    net_arch = [64, 64] if net_arch == 'tiny' else [128, 128]\n",
    "\n",
    "    # Prepare policy kwargs\n",
    "    policy_kwargs = {'net_arch': net_arch, 'activation_fn': activation_fn}\n",
    "\n",
    "    # Convert n_steps from power of 2\n",
    "    best_trial_parameters['n_steps'] = 2 ** best_trial_parameters['n_steps']\n",
    "\n",
    "    # Add policy kwargs\n",
    "    best_trial_parameters['policy_kwargs'] = policy_kwargs\n",
    "    # Add tensorboard logdir\n",
    "    best_trial_parameters['tensorboard_log'] = log_folder\n",
    "    \n",
    "    # Save parameters to a JSON file\n",
    "    path = os.path.join(log_folder, file_path)\n",
    "    with open(path, \"w\") as json_file:\n",
    "        json.dump(best_trial_parameters, json_file, indent=4)\n",
    "\n",
    "    print(f\"Best hyperparameters saved to {file_path}\")  \n",
    "    \n",
    "    return best_trial_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a3e3493-2a48-4a1f-aa09-031cf703a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameters(file_path: str = \"best_hyperparameters.json\") -> Dict:\n",
    "    \"\"\"\n",
    "    Loads previously saved hyperparameters from a JSON file and converts the activation function \n",
    "    string back to its corresponding PyTorch class.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str, optional): The filename from which to load hyperparameters. Default is \"best_hyperparameters.json\".\n",
    "\n",
    "    Returns:\n",
    "    - Dict: A dictionary containing the loaded hyperparameters with correctly formatted values.\n",
    "\n",
    "    Processing Steps:\n",
    "    - Reads the JSON file from the specified path.\n",
    "    - Converts the activation function string (\"Tanh\" or \"ReLU\") back to its PyTorch equivalent (`nn.Tanh` or `nn.ReLU`).\n",
    "    - Returns the fully formatted hyperparameter dictionary.\n",
    "\n",
    "    Notes:\n",
    "    - This function assumes that the JSON file was created using `export_best_parameters()`.\n",
    "    - The `log_folder` must be defined as the directory where the JSON file is stored.\n",
    "    - If the file does not exist or contains invalid data, an exception may be raised.\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "        \n",
    "    # Load JSON data\n",
    "    path = os.path.join(log_folder, file_path)\n",
    "    with open(path, \"r\") as json_file:\n",
    "        hyperparams = json.load(json_file)\n",
    "    \n",
    "    # Convert activation function from string to PyTorch class\n",
    "    activation_fn = hyperparams['policy_kwargs']['activation_fn']\n",
    "    hyperparams['policy_kwargs']['activation_fn'] = {'Tanh': nn.Tanh, 'ReLU': nn.ReLU}[activation_fn]\n",
    "\n",
    "    print(f\"Hyperparameters loaded from {file_path}\")\n",
    "\n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86666c9-df80-478a-b950-9ba7a1fec7dc",
   "metadata": {},
   "source": [
    "### 5.3 Optimization Workflow\n",
    "#### 5.3.1 Trial Evaluation with Callbacks\n",
    "Each trial trained a PPO agent using the sampled hyperparameters and evaluated it periodically using a custom TrialEvalCallback. The callback:\n",
    "\n",
    "- Monitored performance after every 10,000 training steps.\n",
    "- Logged evaluation metrics to Optuna.\n",
    "- Pruned underperforming trials to save computation time. This ensured that only promising hyperparameter configurations were fully trained.\n",
    "\n",
    "#### 5.3.2 Environment Setup for Training & Evaluation\n",
    "To ensure realistic performance evaluation, the environment was initialized with:\n",
    "\n",
    "- Waypoint tracking enabled\n",
    "- Three dynamic targets in head-on, crossing, or static situations\n",
    "- Timescale set to 1/3, meaning each step represented 20 seconds of real-world time\n",
    "- A vectorized environment (n_envs=10) was used for efficient parallel training and evaluation.\n",
    "\n",
    "#### 5.3.3 Hyperparameter Optimization Strategy\n",
    "To efficiently search for the best hyperparameter configurations, Optuna used:\n",
    "\n",
    "- MedianPruner: This pruning strategy stops underperforming trials early based on their intermediate performance relative to the median of all completed trials. It helps allocate resources efficiently by discarding trials unlikely to yield improvements.\n",
    "- TPE Sampler: The Tree-structured Parzen Estimator (TPE) is a Bayesian optimization algorithm that models promising hyperparameter regions based on past trials. Instead of random sampling, it selects hyperparameters more likely to improve performance, leading to faster convergence.\n",
    "\n",
    "\n",
    "By combining pruning and adaptive sampling, the optimization process efficiently explored the hyperparameter space while focusing computational resources on the most promising configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d69ff191-184c-49bc-beb2-cda3241543ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from optuna.pruners import MedianPruner\n",
    "# from optuna.samplers import TPESampler\n",
    "\n",
    "# # Set pytorch num threads to 1 for faster training\n",
    "# torch.set_num_threads(1)\n",
    "# # Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "# sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# # Do not prune before 1/3 of the max budget is used\n",
    "# pruner = MedianPruner(\n",
    "#     n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 2\n",
    "# )\n",
    "# # Create the study and start the hyperparameter optimization\n",
    "# study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "# try:\n",
    "#     study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT, show_progress_bar=True)\n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n",
    "\n",
    "# print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(f\"  Value: {trial.value}\")\n",
    "\n",
    "# print(\"  Params: \")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n",
    "# print(\"  User attrs:\")\n",
    "# for key, value in trial.user_attrs.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n",
    "# # Write report\n",
    "# study.trials_dataframe().to_csv(os.path.join(log_folder, \"study_results_ppo_marineenv.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8fb93-b273-4e5a-b3aa-1fb569442a2a",
   "metadata": {},
   "source": [
    "### 5.4 Results & Best Hyperparameters\n",
    "After 100 trials with 200 000 timesteps each, the best performing PPO configuration was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2144491b-42e7-4e4f-8b09-57715e9b2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_kwargs = export_best_parameters(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc08291c-9cb7-4a6c-bc28-af39c578d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters loaded from best_hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "best_kwargs = load_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aeeaa8c-1e42-4349-baa6-2991460ae49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.00022065441901241827,\n",
       " 'n_steps': 2048,\n",
       " 'batch_size': 256,\n",
       " 'n_epochs': 8,\n",
       " 'gamma': 0.9908682940291461,\n",
       " 'gae_lambda': 0.914493830486739,\n",
       " 'clip_range': 0.20130150169819302,\n",
       " 'ent_coef': 0.0023625133272582688,\n",
       " 'vf_coef': 0.43442214543301433,\n",
       " 'max_grad_norm': 0.7865302683535346,\n",
       " 'target_kl': 0.04389071049636425,\n",
       " 'policy_kwargs': {'net_arch': [64, 64],\n",
       "  'activation_fn': torch.nn.modules.activation.Tanh},\n",
       " 'tensorboard_log': './project_logs'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0a8ce-f261-48b7-aa4a-6cad4914650f",
   "metadata": {},
   "source": [
    "### 5.5 Training the agent with the optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aec97ad3-85ed-4f48-ae96-cc536caecfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=os.path.join(log_folder, 'optuna/best_model/'),\n",
    "    log_path=os.path.join(log_folder, 'optuna/results/'),\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cf84bf8-6622-473e-8eb2-0052aeb0ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_agent = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=train_env,\n",
    "    verbose=0,\n",
    "    device='cpu',\n",
    "    **best_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40326f6a-a102-4b71-aba8-b520271675e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna_agent.learn(total_timesteps=(6e5), reset_num_timesteps=True, progress_bar=True, tb_log_name='optuna/optuna_best', callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5391121-1400-4749-b9e2-b6ea7f3e2f51",
   "metadata": {},
   "source": [
    "Below comparing of the agents using fixed random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5952e657-bb9a-4cd0-8431-60423a022c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 83\n",
      "Seed set to 83\n",
      "Default Model: Mean Reward = 458.91, Std = 445.97\n",
      "Optimized Model: Mean Reward = 736.06, Std = 155.27\n"
     ]
    }
   ],
   "source": [
    "seed = next(seed_generator)\n",
    "env_kwargs = dict(\n",
    "    render_mode=\"rgb_array\",\n",
    "    continuous=True,\n",
    "    training_stage=2,\n",
    "    timescale=1/3,\n",
    "    training=False,\n",
    "    total_targets=3,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Load the default-trained model\n",
    "default_model = PPO.load(os.path.join(log_folder, 'default/best_model/best_model.zip'), device='cpu')\n",
    "default_env = Monitor(gym.make(\"MarineEnv-v0\", **env_kwargs))\n",
    "\n",
    "# Load the optimized model\n",
    "optimized_model = PPO.load(os.path.join(log_folder, 'optuna/best_model/best_model.zip'), device='cpu')\n",
    "optimized_env = Monitor(gym.make(\"MarineEnv-v0\", **env_kwargs))\n",
    "\n",
    "# Evaluate both models\n",
    "mean_default, std_default = evaluate_policy(default_model, default_env, n_eval_episodes=10, deterministic=True)\n",
    "mean_optimized, std_optimized = evaluate_policy(optimized_model, optimized_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"Default Model: Mean Reward = {mean_default:.2f}, Std = {std_default:.2f}\")\n",
    "print(f\"Optimized Model: Mean Reward = {mean_optimized:.2f}, Std = {std_optimized:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be47be23-ccb8-40c5-a451-518d91a91615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record video\n",
    "# run_video_rendering(optimized_model, episodes=10, seed=185, record_video=True, video_folder=video_folder, name_prefix='optimized_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e9cc65f-176c-438b-9ba4-db3b27d1622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_video_rendering(default_model, seed=25, episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e3390-f08f-4f4f-9a25-d8e5db04d25e",
   "metadata": {},
   "source": [
    "### 5.6 Model Evaluation\n",
    "Hyperparameter tuning with Optuna was used to maximize the ASV’s waypoint tracking efficiency and COLREG compliance.\n",
    "\n",
    "TPESampler & Median Pruning helped efficiently search for high-performing configurations.\n",
    "\n",
    "Custom Trial Evaluation Callback ensured continuous assessment and pruning of poor trials.\n",
    "\n",
    "Best PPO configuration was identified after 100 trials, with somehow questionable improvements in navigation and decision-making.\n",
    "\n",
    "The optimized model was then used for final training and deployment, where it consistently navigated toward waypoints while complying with maritime regulations similar to the agent, trained with defaut hyperparameters of the SB3 PPO algorithm.\n",
    "\n",
    "The study results are saved as `csv`, allowing to examine the conducted trials at later stage.\n",
    "\n",
    "**Reading of the training statistics**\n",
    "\n",
    "1. FPS (Frames Per Second) measures the training speed of the agent, indicating how efficiently the environment steps are processed. The optimized model (Optuna) maintains a consistently higher FPS, starting at 4355 and stabilizing around 3490, while the default model starts at 4041 but drops to 2074. This suggests that the optimized hyperparameters improve computational efficiency and training speed.\n",
    "\n",
    "2. Entropy Loss measures the randomness of the agent’s policy; closing to zero indicates less exploration and more deterministic actions. Optimized agent maintaines more negative entropy compared to defaut agent, suggesting better exploration before settling into a more stable policy.\n",
    "\n",
    "3. Explained Variance measures how well the value function predicts returns, with values closer to 1 indicating better predictions.\n",
    "\n",
    "Default model ranges from -4.41e-06 to 0.794, showing strong improvement while optimized model ranges from 0.00053 to 0.741, slightly lower max but more stable.\n",
    "Both models improve prediction quality, but the default model reaches a higher peak accuracy, while the optimized model may be more stable in variance estimation considering the fluctuation during training.\n",
    "Deafut model start with negative value, which could in suggest high initial bias, however it improved and the variance stabalized, which could indicate less bias and controlled variance. Same applies for the optimized model as well but showing better balance.\n",
    "\n",
    "4. Total Loss represents the overall objective function value, including policy and value losses. Default model ranges from 8984.49 ot 118.15, indicating larger fluctuations compared to optimized model with ranges from 3072.49 to 181.71, with significantly lower peak loss.\n",
    "The optimized model has lower total loss and reduced variability, suggesting a more stable and efficient training process.\n",
    "\n",
    "5. Policy Gradient Loss measures how much the policy is changing; more negative values indicate stronger updates. The default model ranges from -0.0068 to -0.0016, while the optimized model ranges from -0.0075 to -0.0026, showing consistently stronger updates. Combined with explained variance, this could mean that the optimized model has lower bias and more controlled updates, leading to a better balance between learning stability.\n",
    "\n",
    "6. Value Loss measures how well the value function approximates expected returns, with lower values indicating better learning stability. The default model has a high range (735.14 to 18,368.38), showing large fluctuations, while the optimized model has a significantly lower range (574.97 to 8,906.79), suggesting more stable learning.\n",
    "\n",
    "7. Approximate KL Divergence measures how much the new policy deviates from the old one, with lower values indicating more stable updates. The default model ranges from 0.0042 to 0.0079, while the optimized model has a slightly lower range 0.0039 to 0.0062, suggesting more conservative policy updates. Thus the optimized model has smoother policy updates, reducing drastic shifts and ensuring more stable learning.\n",
    "\n",
    "8. Clip Fraction represents the proportion of policy updates that exceed the clipping range in PPO, with lower values indicating more stable updates. The default model ranges from 0.0295 to 0.0796, while the optimized model has a lower range (0.0268 to 0.0576), suggesting that the optimized hyperparameters lead to smoother, more controlled updates, reducing excessive policy shifts and improving training stability\n",
    "\n",
    "9. Reward measures the agent's performance, with higher values indicating better policy effectiveness. The default model ranges from -1043.26 to 566.54, while the optimized model achieves a slightly better range of -1039.95 to 594.79, indicating improved learning and higher peak rewards. This suggests that the optimized hyperparameters lead to better overall performance and more efficient policy optimization.\n",
    "\n",
    "### 5.7 Optimization summary\n",
    "The provided video files of both the default and optimized models serve as a visual aid for evaluating the agents' performance. In summary, both models adhere to COLREGs without significant violations. However, in episode 3, the default model fails to reach the waypoint, whereas the optimized model successfully completes the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2720a1-0530-4cd5-93e9-eca8b3c33611",
   "metadata": {},
   "source": [
    "## 6. Follow-Up Work and Future Improvements\n",
    "The current implementation demonstrates a partially COLREG-compliant ASV trained with Deep Reinforcement Learning (DRL) to navigate towards a waypoint while avoiding collisions. However, there are several areas for future work to improve realism, policy effectiveness, and robustness in real-world deployment.\n",
    "\n",
    "### 6.1 Extending COLREG Compliance\n",
    "Currently, the agent follows COLREGs for head-on and crossing situations, but the overtaking rule (Rule 13) has been omitted.\n",
    "\n",
    "**Next step**: Implement full COLREG compliance, including overtaking scenarios where:\n",
    "\n",
    "The ASV recognizes when it is overtaking or being overtaken.\n",
    "The agent adjusts its maneuvers accordingly, ensuring it passes safely.\n",
    "\n",
    "### 6.2 Multi-Agent Reinforcement Learning (MARL)\n",
    "The current approach assumes static or scripted target ship behaviors. However, in reality, vessels react dynamically to COLREG rules.\n",
    "\n",
    "**Next step**: Implement a multi-agent environment where:\n",
    "\n",
    "- Target ships are also reinforcement learning agents, pre-trained on giving way.\n",
    "- The own vessel is trained to \"stand on\" but also learns last-minute evasive actions if the target fails to give way.\n",
    "\n",
    "This setup creates a more realistic simulation, increasing the robustness of the ASV’s policy.\n",
    "\n",
    "### 6.3 Evaluating Different RL Algorithms\n",
    "PPO has been chosen for its stability, but alternative RL algorithms could be tested for better performance:\n",
    "\n",
    "**Next step**: Compare performance of:\n",
    "\n",
    "- Soft Actor-Critic (SAC) → More sample efficient, learns robust policies.\n",
    "- Twin Delayed DDPG (TD3) → Handles continuous actions well, reduces overestimation bias.\n",
    "- Hierarchical RL (HRL) → Train separate policies for waypoint tracking and collision avoidance, then combine them.\n",
    "\n",
    "### 6.4 Incorporating Environment and Ship Dynamics\n",
    "Currently, the environment ignores realistic ship dynamics, assuming an instant response to actions.\n",
    "\n",
    "**Next step**: Add real-world ship dynamics to the environment:\n",
    "\n",
    "- Wind and current effects: The ASV should learn how to compensate for external forces.\n",
    "- Turning rate constraints: The ship's ability to turn depends on speed and rudder angles.\n",
    "- Stopping dynamics: The ASV should account for braking distance.\n",
    "- Engine delay: Speed changes take time to apply, affecting maneuvering.\n",
    "\n",
    "These changes would increase realism but also make learning harder, requiring smarter training strategies.\n",
    "\n",
    "### 6.5 Scaling Up: Larger Environments and Complex Passage Plans\n",
    "Current training involves a single waypoint and a small area.\n",
    "\n",
    "**Next step**:\n",
    "\n",
    "- Train in larger environments with multiple waypoints to simulate full passage plans.\n",
    "- Introduce randomized targets along the route, making the ASV handle longer-term decision-making.\n",
    "- Measure policy effectiveness over long distances and evaluate navigational efficiency.\n",
    "\n",
    "### 6.6 Using Real-World AIS Data for Training\n",
    "The current model relies on synthetic target behaviors, but real-world ship maneuvering follows complex, human-driven decision-making patterns.\n",
    "\n",
    "**Next step**:\n",
    "\n",
    "- Obtain real-world AIS (Automatic Identification System) data from ships.\n",
    "- Train the RL model on real ship behaviors, improving its ability to predict maneuvers.\n",
    "- Test the DRL model in actual maritime environments to see how well it generalizes.\n",
    "\n",
    "### 6.7 Hardware Deployment and Real-World Testing\n",
    "The model is trained in simulation, but real-world deployment requires:\n",
    "\n",
    "**Next step**:\n",
    "\n",
    "- Deploy the trained model onto a real ASV or high-fidelity maritime simulator.\n",
    "- Test how well the RL policy generalizes to real-world conditions.\n",
    "- Implement sensor-based perception (radar, cameras, AIS) instead of synthetic inputs.\n",
    "### 6.8 Funding and Resource Considerations\n",
    "To develop high-fidelity, real-world RL-driven ASVs, significant funding would be required for:\n",
    "\n",
    "- High-performance computing resources for large-scale DRL training.\n",
    "- Access to AIS and real ship maneuver data.\n",
    "- Developing real-world test environments, including physical ASVs for deployment.\n",
    "- Collaboration with maritime institutions for regulatory compliance and testing.\n",
    "\n",
    "Potential funding sources include:\n",
    "\n",
    "- Government maritime research grants.\n",
    "- Private sector funding from shipping and AI companies.\n",
    "- Collaborations with academia and navies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2aa6a8-62e4-4b9f-a4cd-16b366d06a80",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "The developed PPO-based autonomous navigation model demonstrates high performance in the given environment, efficiently following COLREGs while maintaining an optimal route to the waypoint. The agent successfully balances compliance with collision avoidance rules and ETA accuracy, adjusting its speed only when necessary to avoid excessive deviations.\n",
    "\n",
    "One of the key strengths of the model is its scalability. Since the observation space is based on relative bearings and distances rather than absolute latitude/longitude coordinates, the model can be easily extended to handle more dynamic targets without requiring major architectural changes. This flexibility makes it well-suited for real-world maritime navigation scenarios involving multiple vessels.\n",
    "\n",
    "Despite its strong performance, reward function optimization remains an area for improvement. Refining the reward structure could further enhance decision-making efficiency, particularly in scenarios requiring complex maneuvers or long-term planning. Additionally, adaptive exploration strategies could help the model generalize better to varying traffic densities and environmental conditions.\n",
    "\n",
    "Overall, this study demonstrates the viability of reinforcement learning for autonomous marine navigation, providing a robust foundation for future enhancements such as multi-agent coordination, dynamic weather adaptation, and real-world sensor integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448a849-38c7-4e09-b185-bd23343fb02e",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "In Chapter 2: Previous Research, a review of relevant academic articles has been provided and will not be listed here. Below are the primary sources and tools referenced throughout this project:\n",
    "\n",
    "- International Maritime Organization (IMO) & COLREGs – The International Regulations for Preventing Collisions at Sea, which define the legal framework for vessel navigation and avoidance maneuvers.\n",
    "- PyTorch – Open-source deep learning framework used for implementing and training the reinforcement learning model.\n",
    "- Hugging Face – Provides tools and model repositories for reinforcement learning and deep learning applications.\n",
    "- Stable-Baselines3 (SB3) – A reliable reinforcement learning library built on PyTorch, used for training the PPO agent.\n",
    "- Optuna – A hyperparameter optimization framework that was utilized for fine-tuning the reinforcement learning model, improving efficiency and performance.\n",
    "- Gymnasium (formerly OpenAI Gym) – A toolkit for developing and comparing reinforcement learning algorithms, used to create and test the training environment.\n",
    "- OpenAI – Provided foundational research and tools in deep reinforcement learning, contributing to the development of stable baselines and policy optimization techniques.\n",
    "- ChatGPT – Assisted in structuring explanations, refining technical documentation, and improving clarity in methodology and findings.\n",
    "  \n",
    "These sources have been instrumental in the development and implementation of the model, ensuring both compliance with maritime navigation rules and best practices in reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
